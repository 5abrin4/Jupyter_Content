{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised, Unsupervised and Reinforcement Learning\n",
    "\n",
    "One of the ways of differentiating categories of machine learning algorithms is to split them into **supervised**, **unsupervised** and **reinforcement** learning methods. \n",
    "\n",
    "With **supervised** methods, the training data come with the outcome you are trying to predict--they are labeled. For example, with linear regression, we have linked input and output values and are predicting the function that relates them. With image classification, we might start with a set of images that are labeled as cats, dogs, people, etc., and then train the algorithm to learn to classify a new image into these classes.\n",
    "\n",
    "With **unsupervised** methods, we have unlabeled data (no y-values or pre-defined categories) and ask the algorithm to learn patterns from the data itself. These methods usually try to categorize the observations.\n",
    "\n",
    "With **reinforcement learning**, the training data is also unlabeled, but the system received feedback for its actions/categorizations. Game systems are a classic example of this--learn to play chess by playing games and learning what moves lead to a winning strategy. Robotics and autonomous vehicles are other applications--learn how to climb stairs by getting points for actions that leads to going up stairs.\n",
    "\n",
    "There are also methods that mix these.\n",
    "\n",
    "## Types of Supervised Learning\n",
    "\n",
    "The two main types of supervised learning are **classification** and **regression**.\n",
    "\n",
    "In **classification**, the response variable (y) is categorical or discrete. For example, this image is a cat, dog, bird, etc.; you do or do not have some disease; this plant is a desired crop or a weed to kill; etc.\n",
    "\n",
    "In **regression**, the response variable (y) is continuous or numerical. in the automobile fuel efficiency data we've been working with, miles per gallon is the response variable and the regression approaches we've been looking at are finding the model that takes the input (hp) and predicts the output (mpg).\n",
    "\n",
    "The image below from the [maplearn.ml package documentation](https://maplearn.readthedocs.io/en/latest/maplearn.ml.html) provides a nice visual of the general idea of classification and regression.\n",
    "\n",
    "![Classification vs regression. Image from maplearn.ml documentation](images/classif_reg.maplearn.png)\n",
    "\n",
    "\n",
    "## Types of classification\n",
    "\n",
    "Many methods have been developed for classification and the list below is certainly not comprehensive. Nor can we hope to cover all of them in this class. But hopefully this list gives you a sense of the diversity and places to start looking.\n",
    "\n",
    "* Linear Classifiers\n",
    "  * Logistic regression\n",
    "  * Naive Bayes classifier\n",
    "  * Fisherâ€™s linear discriminant\n",
    "* Kernel methods\n",
    "  * Support vector machines\n",
    "  * Least squares support vector machines\n",
    "* k-nearest neighbors\n",
    "* Decision trees\n",
    "  * Random forests\n",
    "* Neural networks\n",
    "* Learning vector quantization\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Despite its name, **logistic regression** is typically used in machine learning as a **classification** algorithm. Logistic regression estimates the probability of belonging to each class, given the values of the predictor variables (features). This is then typically used, in conjunction with a threshold to predict discrete (most often binary) class association--1/0, yes/no, true/false--based on input data. Logistic regression predicts the probability of occurrence of an event by fitting a **logit function**. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common example of logistic regression is the issue of classifying email as spam or not-spam. I found this [Introduction to Logistic Regression](https://courses.lumenlearning.com/introstats1/chapter/introduction-to-logistic-regression/) particularly helpful in preparing this.\n",
    "\n",
    "Logistic regression is a generalized linear model where the outcome is a binary (two-levels) categorical variable. What we are looking for is actually **probabilities**: What is the probability of each possible outcome? Outcome 1 (spam) happens with the probability of $p_i$ and outcome 0 (not-spam) with the probability $1-p_i$.\n",
    "\n",
    "The **odds** of an event are the ratio of the probability that the event occurs to the probability that it doesn't. \n",
    "\n",
    "$$ odds\\,of\\,event = \\frac{p}{1-p} $$\n",
    "\n",
    "Remember our standard linear regression equation:\n",
    "\n",
    "$$ \\hat{y} = \\beta_0  + \\beta_1x_1 + \\beta_2x_2 ... \\beta_ix_i$$\n",
    "\n",
    "In this, $\\hat{y}$ can have any range. To get a probability, we need to transform $\\hat{y}$ such that it has values between 0 and 1. A common transformation is the **logit transformation**.\n",
    "\n",
    "$$ logit(y) = ln(odds) = ln(\\frac{p_i}{1-p_i}) $$\n",
    "\n",
    "So we end up with:\n",
    "\n",
    "$$  ln(\\frac{p_i}{1-p_i}) = \\beta_0  + \\beta_1x_1 + \\beta_2x_2 ... \\beta_ix_i$$  \n",
    "\n",
    "To solve for $p$, we can take the antilog and do a bunch of algebra to end up with:\n",
    "\n",
    "$$ p = \\frac{1}{1+e^{-(\\beta_1x_1 + \\beta_2x_2 ... \\beta_ix_i)}} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Sigmoid Function\n",
    "\n",
    "Logistic regression makes use of the **sigmoid function** pictured below. The sigmoid function has values very close to (but not equal to) 0 or 1 for most of its distribution. \n",
    "\n",
    "$$ \n",
    "    \\sigma(x) = \\frac{1}{1+e^{-x}} = \\frac{1}{1+exp^{-x}}\\\\\n",
    "$$\n",
    "And think of $\\sigma(x)$ as *y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to generate sigmoid graph below\n",
    "import plotnine as pn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "x = np.linspace(-10,10, num=100)\n",
    "y = 1/(1+np.exp(-x))\n",
    "\n",
    "df = pd.DataFrame(x,y)\n",
    "\n",
    "pn.ggplot(df, pn.aes(x='x', y='y')) + pn.geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to find the **logistic regression function**, such that the predicted responses are as close to the **actual response**, for each observation. Remember that this is a binary response, 0 or 1, but the output of the logistic regression model is a _probability_, not a class prediction. Thus, the requirement that the output be between 0 and 1 is because it is a probability, not because our class labels are 0 and 1.\n",
    "\n",
    "### Logistic Regression Methodology\n",
    "\n",
    "Logistic regression determines the **best predicted weights**, $b_0, b_1,..., b_i$, such that the function is as close as possible to all actual (observed) responses. Calculating the best weights using training data is called model training or fitting.\n",
    "\n",
    "\n",
    "### Single Variable Logistic Regression example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Let's make some data to work with\n",
    "\n",
    "\n",
    "# Question what are these data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of options for the [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier. For this example, we will use the `liblinear` solver--the algorithm used in the optimization step. This is mostly for two reasons...1) until recently this was the default(*lbfgs* is now default--[here is](https://towardsdatascience.com/dont-sweat-the-solver-stuff-aea7cddc3451) a good article on the options) and 2) it produces some \"interesting\" results to discuss. Certainly look at the options and play with different solvers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a model using our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the model parameters\n",
    "\n",
    "print(\"Classes:\", model['logistic'].classes_)\n",
    "print(\"Intercept:\", model['logistic'].intercept_)\n",
    "print(\"Coefficients:\", model['logistic'].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log odds\n",
    "\n",
    "Note that the coefficient here is also referred to as the log odds score or effect. For every one unit change in x, in this case, the odds of positivity increase by 0.51.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the model do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output matrix above, each row corresponds to a single observation ($x$). The first column is the predicted probability of the output being the  $0^{th}$ class-- 0 or $(1-p(x))$. The second column is the predicted probability of class 1, $p(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The above example and image below come from the Real Python blog [*Logistic Regression in Python*](https://realpython.com/logistic-regression-python/).\n",
    "\n",
    "![](images/log-reg-5_real_python.png)\n",
    "\n",
    "The green circles represent the actual responses ($y$) that are correctly predicted by the model. The red x shows the incorrect prediction. The solid line is the estimated logistic function $p(x)$, and the grey squares along that line are the predicted responses (second column in table). The black dashed line is the logit, $f(x)$.\n",
    "\n",
    "As the plot above and the `model.predict(x)` output show, the model correctly classifies nine of the 10 observations. As such, the accuracy is 9/10=0.9, which is available with the `model.score()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification metrics\n",
    "\n",
    "The `model.score()` tells us something about our model's performance, but not really everything we would like to know. \n",
    "\n",
    "Taking a different classification problem, let's look at a model that classifies images as either cat or not cat. Here are common terms used to describe classification success:\n",
    "\n",
    "![Cat classification success](images/classification_success.png)\n",
    "\n",
    "One way of gaining insight into our model is what is called a **confusion matrix**. This summarizes the true and false positive and negative counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "What kind of error is our model making?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Classification Metrics\n",
    "\n",
    "For the following, we will refer to the 0 class as negative and the 1 class as positive. \n",
    "\n",
    "**Precision**: The ability of the classifier not to label as positive (1) a sample that is negative (0). Out of those predicted positive, how many are actual positive.\n",
    "\n",
    "$$ Precision =  \\frac{True\\,Positive}{True\\,Positive\\,+\\,False\\,Positive} =  \\frac{True\\,Positive}{Total\\,Predicted\\,Positive} $$\n",
    "\n",
    "Precision is a good measure when the cost of False Positive is high. E.g. Email spam filter. If lots of email is falsely flagged as spam and discarded, you might miss important emails. Precision is also called the *positive predictive value*. There is also a *negative predictive value* that is the ratio of true negatives over the sum of true and false negatives.\n",
    "\n",
    "**Recall/Sensitivity**:  The ability of the classifier to find all the positive (1) samples. How many of the actually positives does the model label as positive.\n",
    "\n",
    "$$ Recall = \\frac{True\\,Positive}{True\\,Positive + False\\,Negative} =  \\frac{True\\,Positive}{Total\\,Actual\\,Positive} $$\n",
    "\n",
    "Recall is a good measure when the cost of False Negative is high. E.g. disease diagnosis. If a sick patient is predicted as not sick, the cost associated with a false negative is high. Recall is also called *sensitivity* or *True Positive Rate* (**TPR**).\n",
    "\n",
    "**Specificity**: The ability of the classifier to fine all the negative (0) samples. How many of the actually negatives does the model label as negative.\n",
    "\n",
    "$$ Specificity = \\frac{True\\,Negative}{True\\,Negative + False\\,Positive} = \\frac{True\\,Negative}{Total\\,Actual\\,Negative} $$\n",
    "\n",
    "Since we'll use this later, 1 - Specificity is known as the *False Positivity Rate (**FPR**)*.\n",
    "\n",
    "**F1-Score** also known as F-beta: A weighted harmonic mean of the precision and recall, where and F-beta score reaches its best value at 1 and worst score at 0. \n",
    "\n",
    "\n",
    "The F-beta score weighs recall more than precision by a factor of `beta`. `beta=1.0` means recall and precision are equally important. \n",
    "\n",
    "\n",
    "$$ F1 = 2\\frac{Precision\\,*\\,Recall}{Precision\\,+\\,Recall} $$\n",
    "\n",
    "** Support**: The number of samples in each class.\n",
    "  \n",
    "**Accuracy**: Total number of correct predictions over all predictions. Only accurate with balanced models (relatively equal frequency of 0/1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another set of classification metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression example\n",
    "\n",
    "For this example, let's predict if a patient will develop (1) diabetes or not (0). We have several variables that can be used for this prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotnine as pn\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#diabetes = pd.read_csv(\"data/diabetes.csv\")\n",
    "diabetes = pd.read_csv(\"/blue/zoo6927/share/Jupyter_Content/data/diabetes.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup train/test data\n",
    "\n",
    "# Get the y values and drop from df\n",
    "\n",
    "\n",
    "# Split to 75% train, 25% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a model using our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "\n",
    "# Predict ys using model and test data\n",
    "\n",
    "# Get model.score for both training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred), display_labels=model['logistic'].classes_).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['logistic'].coef_)\n",
    "\n",
    "pd.DataFrame(np.transpose(model['logistic'].coef_), x.columns, columns=['coef']).sort_values(by='coef', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operator Characteristic (ROC) Curve and Area Under the Curve (AUC)\n",
    "\n",
    "Originating [rating radar operators in the 1940s](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#History), a ROC curve is a way to visualize the tradeoff between sensitivity and specificity.\n",
    "\n",
    "So far, we have been assuming anything below 0.5 is class 0 and anything about 0.5 is class 1. But that is one of any number of cutoffs and we could change the classification threshold.\n",
    "\n",
    "**What effect would having a lower threshold have on false positive and negative rates?**\n",
    "\n",
    "**When might you want to have more false positives? or more false positives?**\n",
    "\n",
    "Remember:\n",
    "* Recall = Sensitivity = TPR = true positives / total actual positives\n",
    "* Specificity = true negatives / total actual negatives\n",
    "  * 1 - Specificity = FPR\n",
    "\n",
    "The ROC curve plots these against each other at classification thresholds from 0 to 1. Often a line for a random model (randomly classify observations as 0/1--should be 50% accurate) is also added.\n",
    "\n",
    "The **Area Under the Curve (AUC)** is an aggregate measure of model performance across all classification thresholds. \n",
    "\n",
    "The higher the AUC, the better the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "\n",
    "y_pred_proba = model.predict_proba(x_test)[::,1]\n",
    "fpr, tpr, thresh = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Plot ROC w/ RocCurveDisplay\n",
    "#display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc,                                  estimator_name='example estimator')\n",
    "#display.plot() \n",
    "\n",
    "# Plot ROC w/ plotnine\n",
    "df= pd.DataFrame(fpr,tpr)\n",
    "pn.ggplot(df, pn.aes(x='fpr', y='tpr')) + pn.geom_line() + pn.geom_text(pn.aes(x= 0.5, y= 0.1, label=auc)) + pn.geom_abline(intercept=0, slope=1, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image below is from the great article by Ruchi Toshniwal [Demystifying ROC Curves](https://towardsdatascience.com/demystifying-roc-curves-df809474529a) and shows an example ROC curve color coded to also show the threshold value.\n",
    "\n",
    "![ROC Curve color coded bt threshold from Ruchi Toshniwal](images/ROC_curve_thresholds.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (full)",
   "language": "python",
   "name": "python3-3.8-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
